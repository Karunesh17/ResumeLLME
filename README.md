<<<<<<< HEAD
# ResumeLLMe 

---

![](./.github/images/Preview.png)

ðŸ‘‰ Check some sample rÃ©sumÃ©s generated by ResuLLMe ([1](./.github/samples/Simple.pdf), [2](./.github/samples/Alta.pdf), [3](./.github/samples/Awesome.pdf))

## ðŸš€ Concept

ResumeLLMe is a prototype that uses Large Language Models (LLMs) to tailor rÃ©sumÃ©s. It's goal is to enhance rÃ©sumÃ©s to help candidates avoid common mistakes that occur while applying for jobs. It is like a smart career advisor to check your rÃ©sumÃ©.

You can use ResuLLMe live at [https://resumellme.streamlit.app/]

## ðŸ›  How It Works
ResumeLLMe now supports both OpenAI and Gemini, empowering the application to enhance rÃ©sumÃ©s with even more powerful and diverse language models, providing users with smarter, more accurate career guidance.  

ResumeLLMe receives your previous CV as a PDF or Word Document. Then, it uses LLMs to:
* Improve the rÃ©sumÃ© following published rÃ©sumÃ© guidelines by well-reputed schools
* Convert the rÃ©sumÃ©s to a JSON Resume format
* Render the JSON resume using LaTeX to generate a new PDF of the enhanced resume

## ðŸƒ Running

To run ResumeLLMe locally, the simplest way is to use Docker:

```
docker-compose up -d
```

This will make the app avaialable at [`https://localhost:8501/`](https://localhost:8501/)

### ðŸª„ Installation Instructions for Running Natively

To run the app without Docker, you will need to install two things for the app to work. The first item is to install the Python dependencies:

```
pip install -r requirements.txt
```

The second item is to install the LaTeX packages:

```
xargs sudo apt install -y < packages.txt
```

Lastly, to run ResuLLMe locally, execute:

```
streamlit run src/Main.py
```

=======
# ResumeLLME
>>>>>>> f4f72138428c0ee4bb8ba8735960622b6a56262f
